---
title: "Tau Vulnerability Report"
author:
  - name: "Emir Turkes"
  - name: "Columbia University"
date: '`r strftime(Sys.time(), format = "%B %d, %Y")`'
bibliography: "../tau-vulnerability.bib"
biblio-style: apalike
link-citations: true
output:
  html_document:
    number_sections: true
    theme: lumen
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
knit:
  (function(inputFile, encoding) {
    rmarkdown::render(
      inputFile, encoding = encoding, output_file = "../results/tau-vulnerability-report.html")})
---

```{r, include = FALSE}
#    This file is part of tau-vulnerability.
#    Copyright (C) 2019  Emir Turkes
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    Emir Turkes can be contacted at emir.turkes@eturkes.com

knitr::opts_chunk$set(fig.width = 8.5, fig.height = 7, error = TRUE)
```

<style type="text/css">
body {font-size: 16px;}
h1.title {font-size: 35px;}
h1 {font-size: 24px;}
h2 {font-size: 22px;}
h3 {font-size: 20px;}
.toc-content {padding-left: 0px; padding-right: 0px;}
div.tocify {width: 100%;}
.tocify-subheader .tocify-item {font-size: 0.95em; padding-left: 25px; text-indent: 0;}
div.main-container {max-width: none; width: 100%;}
</style>

*This is an in-depth analysis to characterize differential vulnerability to tauopathy.*

The background for this data is as follows:

This analysis was performed in R except where noted.
The source code and instructions for rerunning the analysis can be found at [github.com/eturkes/tau-vulnerability](https://github.com/eturkes/tau-vulnerability).

# Final Results

**Read just this section for the final results of the analysis and a summary of the methods.**

# ~~~ Breakdown of Methods ~~~ {-}

The following top-level sections break down the methods used to perform the analysis and only needs to be read if one is interested.
We start by setting some global variables and loading in any required packages.

```{r}
library(conflicted)
library(devtools)
library(ggplot2)
library(SingleCellExperiment)
library(data.table)
library(DT)
library(scran)
library(scater)
library(BiocSingular)
library(svd)
library(Rtsne)
library(S4Vectors)
library(SC3)
library(gprofiler2)
```

```{r}
assets_dir <- file.path(getwd(), "..", "assets")
results_dir <- file.path(getwd(), "..", "results")

ggplot_custom = function(data, x, y, col, type) {
	gg <- ggplot(data, aes_string(x = x, y = y)) +
		geom_point(size = 0.2, alpha = 0.6, aes_string(col = col)) +
	  theme_classic() + theme(legend.position = "bottom")
	if (type == "cont") {
		gg <- gg + scale_colour_gradient(low = "blue", high = "red")
	} else if (type == "cat") {
		gg <- gg + guides(color = guide_legend(override.aes = list(size = 3)))
	}
	gg}

organise_marker_genes <- function(object, k, p_val, auroc) {
    dat <- rowData(object)[, c(paste0("sc3_", k, "_markers_clusts"), paste0("sc3_", k, 
        "_markers_auroc"), paste0("sc3_", k, "_markers_padj"), "feature_symbol")]
    dat <- dat[dat[, paste0("sc3_", k, "_markers_padj")] < p_val & !is.na(dat[, paste0("sc3_", 
        k, "_markers_padj")]), ]
    dat <- dat[dat[, paste0("sc3_", k, "_markers_auroc")] > auroc, ]
    
    d <- NULL
    
    for (i in sort(unique(dat[, paste0("sc3_", k, "_markers_clusts")]))) {
        tmp <- dat[dat[, paste0("sc3_", k, "_markers_clusts")] == i, ]
        tmp <- tmp[order(tmp[, paste0("sc3_", k, "_markers_auroc")], decreasing = TRUE), ]
        d <- rbind(d, tmp)
    }
    
    if(nrow(dat) > 0) {
        return(d)
    } else {
        return(NULL)
    }
}
```

# Original Data

We start by reading in data from previous broad analyses and displaying their final results.

## Habib 2017 snRNAseq

The cleaned dataset here is derived from the analysis in [github.com/eturkes/tau-vulnerability](https://github.com/eturkes/habib-2017-snRNAseq) *(currently private)* and the original publication @habib_massively_2017.

```{r}
habib_2017_sce <- readRDS(file.path(assets_dir, "habib_2017_sce.rds"))
habib_2017_df <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type")],
  stringsAsFactors = FALSE)

ggplot_custom(
  data = habib_2017_df, x = "habib_tsne1", y = "habib_tsne2",
  col = "habib_cluster_name", type = "cat")
rm(habib_2017_df)
```

# Ex/In Neurons of the PFC

We start by looking in the PFC region and performing deeper clustering on both excitatory and inhibitory neurons.

```{r}
# Perform subsetting.
habib_2017_sce <- readRDS(file.path(assets_dir, "habib_2017_sce.rds"))
regions <- "PFC"
cell_types <- "exPFC1|exPFC2|GABA1|GABA2"
keep <- grepl(regions, colData(habib_2017_sce)$cell_id_stem)
habib_2017_sce <- habib_2017_sce[, keep]
keep <- grepl(cell_types, colData(habib_2017_sce)$habib_cluster_name)
habib_2017_sce <- habib_2017_sce[, keep]

# Perform PCA.
new_trend <- makeTechTrend(x = habib_2017_sce)
fit <- trendVar(habib_2017_sce, use.spikes = FALSE, loess.args = list(span = 0.05))
fit$trend <- new_trend
dec <- decomposeVar(fit = fit)
rm(fit)
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache1", "habib_2017_pca.rds")
if (file.exists(rds)) {
  habib_2017_pca <- readRDS(rds)
} else {
  habib_2017_pca <- denoisePCA(habib_2017_sce, technical = new_trend, BSPARAM = IrlbaParam())
  saveRDS(habib_2017_pca, rds)}
```

```{r}
habib_2017_sce <- habib_2017_pca
rm(habib_2017_pca)

GLUT <- grepl("GLUT", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GLUT, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GLUT <- as.data.frame(assays(sub_sce)$logcounts)
GLUT <- melt(GLUT, value.name = "GLUT_logcounts")

SATB2 <- grepl("SATB2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[SATB2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
SATB2 <- as.data.frame(assays(sub_sce)$logcounts)
SATB2 <- melt(SATB2, value.name = "SATB2_logcounts")

GRIA1 <- grepl("GRIA1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA1 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA1 <- melt(GRIA1, value.name = "GRIA1_logcounts")

GRIA2 <- grepl("GRIA2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA2 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA2 <- melt(GRIA2, value.name = "GRIA2_logcounts")

WFS1 <- grepl("WFS1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[WFS1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
WFS1 <- as.data.frame(assays(sub_sce)$logcounts)
WFS1 <- melt(WFS1, value.name = "WFS1_logcounts")

TBR1 <- grepl("TBR1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[TBR1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
TBR1 <- as.data.frame(assays(sub_sce)$logcounts)
TBR1 <- melt(TBR1, value.name = "TBR1_logcounts")

rm(sub_sce)

df_redDim <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type")],
  reducedDim(habib_2017_sce, "PCA")[ , 1:2],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  stringsAsFactors = FALSE)
rownames(df_redDim) <- NULL
plotPCA(habib_2017_sce, ncomponents = 3, colour_by = "log10_total_features_by_counts")
```

## Dimensionality Reduction {.tabset}

We use several approaches to identify clusters using tSNE, so this section is broken down into multiple subsections, with each tab referring to a particular set of coordinates.

### Habib tSNE

As tSNE coordinates were released with the original dataset, we plot them here to replicate the results of @habib_massively_2017.

```{r}
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cell_type", type = "cat")
```

### All Genes tSNE

We also calculate and plot new tSNE coordinates using all of the genes in the subsetted dataset.

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache1", "habib_2017_runTSNE.rds")
if (file.exists(rds)) {
  habib_2017_runTSNE <- readRDS(rds)
} else {
  habib_2017_runTSNE <- runTSNE(habib_2017_sce, use_dimred = "PCA", perplexity = 30, rand_seed = 100)
  saveRDS(habib_2017_runTSNE, rds)}
```

```{r}
habib_2017_sce <- habib_2017_runTSNE
tmp_df <- data.frame(reducedDim(habib_2017_sce, "TSNE"), stringsAsFactors = FALSE)
rownames(tmp_df) <- NULL
names(tmp_df) <- paste0("new_tsne", 1:2)
df_redDim <- data.frame(df_redDim, tmp_df, stringsAsFactors = FALSE)

ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cell_type", type = "cat")
rm(df_redDim, tmp_df, habib_2017_runTSNE)
```

### HVG tSNE

We now cluster using the top 1,000 HVGs.

```{r}
# Select the top 1,000 HVGs.
dec1 <- dec
dec1$bio[which(dec$bio < 1e-5)] <- 1e-5
dec1$FDR[which(dec$FDR < 1e-100)] <- 1e-100
w2kp <- which(dec$FDR < 1e-10 & dec$bio > 0.02)
rm(dec, dec1)
habib_2017_sce_hvg <- habib_2017_sce[w2kp, ]
edat <- t(as.matrix(logcounts(habib_2017_sce_hvg)))
edat <- scale(edat)
```

```{r, cache = TRUE}
# Perform PCA.
rds <- file.path(assets_dir, "cache1", "habib_2017_ppk.rds")
if (file.exists(rds)) {
  habib_2017_ppk <- readRDS(rds)
} else {
  habib_2017_ppk <- propack.svd(edat, neig = 50)
  saveRDS(habib_2017_ppk, rds)}
```

```{r}
pca <- t(habib_2017_ppk$d*t(habib_2017_ppk$u))
rm(edat, habib_2017_ppk)
tmp_df <- data.frame(pca[ , 1:2], stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_pc", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type"
  )],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  tmp_df,
  stringsAsFactors = FALSE)
rownames(habib_2017_df_hvg) <- NULL
```

```{r, cache = TRUE}
# Perform tSNE.
rds <- file.path(assets_dir, "cache1", "habib_2017_Rtsne.rds")
if (file.exists(rds)) {
  habib_2017_Rtsne <- readRDS(rds)
} else {
  set.seed(100)
  habib_2017_Rtsne <- Rtsne(pca, pca = FALSE, perplexity = 30)
  saveRDS(habib_2017_Rtsne, rds)}
```

```{r}
tmp_df <- data.frame(habib_2017_Rtsne$Y, stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_tsne", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(habib_2017_df_hvg, tmp_df, stringsAsFactors = FALSE)
reducedDims(habib_2017_sce_hvg) <- SimpleList(PCA = pca, TSNE = habib_2017_Rtsne$Y)
rm(habib_2017_Rtsne, pca, tmp_df)

ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cell_type", type = "cat")
```

## Clustering

### k-means

We apply a k-means method atop the 50 PCs generated earlier, choosing to generate 2-4 clusters.

```{r}
all_num_clust <- c(2:4)
habib_2017_df_hvg <- habib_2017_df_hvg[ , !grepl("^KM_", names(habib_2017_df_hvg))]
rowData(habib_2017_sce_hvg)$feature_symbol <- rowData(habib_2017_sce_hvg)$external_gene_name
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache1", "habib_2017_km_label.rds")
if (file.exists(rds)) {
  km_label <- readRDS(rds)
  habib_2017_df_hvg <- readRDS(file.path(assets_dir, "cache1", "habib_2017_df_hvg1.rds"))
} else {
  for (num_clust in all_num_clust) {
    cat(paste0("k-means with ", num_clust, " clusters.\n"))
    kmeans_out <- kmeans(
      reducedDim(habib_2017_sce_hvg, "PCA"), centers = num_clust, iter.max = 1e8,
      nstart = 2500, algorithm = "MacQueen")
    habib_2017_km_label <- paste0("km_", num_clust, "_clusters")
    habib_2017_df_hvg[[habib_2017_km_label]] = as.factor(kmeans_out$cluster)
    rm(kmeans_out)}
  saveRDS(habib_2017_km_label, rds)
  saveRDS(habib_2017_df_hvg, file.path(assets_dir, "cache1", "habib_2017_df_hvg1.rds"))}
```

### SC3

As an alternative method, we generate another set of 2-4 clusters using SC3.

```{r}
rowData(habib_2017_sce)$feature_symbol <- rowData(habib_2017_sce)$external_gene_name
counts(habib_2017_sce) <- as.matrix(counts(habib_2017_sce))
logcounts(habib_2017_sce) <- as.matrix(logcounts(habib_2017_sce))
all_ks <- c(2:4)
p_data <- c("habib_cluster_name", "cell_id_stem")
```

```{r, cache = TRUE}
habib_2017_sce <- sc3(habib_2017_sce, ks = all_ks, biology = TRUE)
```

```{r}
rds <- file.path(assets_dir, "cache1", "habib_2017_df_hvg2.rds")
if (file.exists(rds)) {
  habib_2017_df_hvg <- readRDS(rds)
  habib_2017_sce <- readRDS(file.path(assets_dir, "cache1", "habib_2017_sce.rds"))
} else {
  for (one_ks in all_ks) {
    sc3_label <- paste0("sc3_", one_ks, "_clusters")
    habib_2017_df_hvg[[sc3_label]] <- as.factor(colData(habib_2017_sce)[, sc3_label])}
  saveRDS(habib_2017_df_hvg, rds)
  saveRDS(habib_2017_sce, file.path(assets_dir, "cache1", "habib_2017_sce.rds"))}
```

## Cluster Mapping {.tabset}

Now we visualize the correlation between our new cluster set as well as the original clusters in @habib_massively_2017.
We start by reading in Supplementary Table 7 (nmeth.4407-S10.xlsx) from @habib_massively_2017 to provide a consistent set of labels and merging it into our data.

```{r}
# Define custom functions.
name_change <- function(data, orig_name, new_name) {
	index = which(names(data) == orig_name)
	if (length(index) > 0) {
		names(data)[index] = new_name
	}
	data}

custom_merge = function(x, y, mess = NULL, ...) {
	if (!is.null(mess)) {
		intersect_vars = paste(intersect(names(x), names(y)), collapse = ", ")
		cat(paste0("Merging dataframes on variables = { ", intersect_vars, " }\n"))
	}
	merge(x, y, by = intersect(names(x), names(y)), ...)}

# Read in supplemental information.
tmp_lab <- read.table(
  file.path(assets_dir, "cluster-num-label.txt"), sep = "\t", header = TRUE, stringsAsFactors = FALSE)
tmp_lab <- name_change(tmp_lab, "Name", "habib_cluster_name")
tmp_lab <- name_change(tmp_lab, "Name.1", "habib_cell_type")
tmp_lab <- name_change(tmp_lab, "Cell.ID", "sample")
tmp_lab <- name_change(tmp_lab, "Cell.Type.ID", "habib_cluster")

tmp_clust <- read.table(
  file.path(assets_dir, "paper-cluster.txt"), sep = "\t", header = TRUE,
  comment.char = "", stringsAsFactors = FALSE)
tmp_clust <- name_change(tmp_clust, "X.Genes", "n_genes")
tmp_clust <- name_change(tmp_clust, "X.Transcripts", "n_transcripts")
tmp_clust <- name_change(tmp_clust, "Cell.ID", "sample")
tmp_clust <- name_change(tmp_clust, "Cluster.ID", "habib_cluster")
tmp_clust <- name_change(tmp_clust, "Cluster.Name", "habib_cluster_name")

tmp_clust <- custom_merge(
  tmp_clust, tmp_lab[ , c("habib_cluster_name", "habib_cell_type")], all.x = TRUE)

# Merge into our data.
habib_2017_all_clust <- custom_merge(habib_2017_df_hvg, tmp_clust)

sc3_clust <- data.frame(
  colData(habib_2017_sce)[ , paste0("sc3_", all_ks, "_clusters")], stringsAsFactors = FALSE)
for (ks in all_ks) {
  sc3_label <- paste0("sc3_", ks, "_clusters")
  sc3_clust[ , sc3_label] <- as.factor(sc3_clust[ , sc3_label])}
sc3_clust$sample <- colnames(habib_2017_sce)

habib_2017_all_clust <- custom_merge(habib_2017_all_clust, sc3_clust)
habib_2017_all_clust <- data.frame(
  habib_2017_all_clust, GLUT["GLUT_logcounts"], SATB2["SATB2_logcounts"], GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"], WFS1["WFS1_logcounts"], TBR1["TBR1_logcounts"], stringsAsFactors = FALSE)
rm(habib_2017_df_hvg, tmp_clust, tmp_lab, sc3_clust, GLUT, GRIA1, GRIA2, SATB2, TBR1, WFS1)
```

We visualize our clusters using tSNE, looking at both the HVG tSNE generated earlier and coloring our clustering results atop of the clusters in @habib_massively_2017.

```{r}
all_vars <- c(
  "habib_cluster_name", "habib_cell_type",
  paste0("km_", all_ks, "_clusters"), paste0("sc3_", all_ks, "_clusters"))
genes <- c(
  "GLUT_logcounts", "SATB2_logcounts", "GRIA1_logcounts",
  "GRIA2_logcounts", "WFS1_logcounts", "TBR1_logcounts")
```

### HVG tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cont"))}
```

### Habib tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cont"))}
```

### Consensus Matrix

We can also assess the performance of our SC3 clusters using a correlation matrix.
In an ideal situation, diagonal blocks will be completely red while off-diagonal elements are completely blue.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_consensus(habib_2017_sce, k = k, show_pdata = p_data)}
```

The diagonality of the concensus matrix can be quantitatively measured using a silhouette plot, where 1 represents a perfectly block-diagonal consensus matrix.

```{r}
# Commented out for now due to errors.
# for (k in all_ks) {
#   sc3_plot_silhouette(habib_2017_sce, k = k)}
```

## Differential Expression

In this section, we calculate differentially expressed genes (DEGs) between the newly created clusters, eventually using this data to assign marker genes to each cluster.

### Expression Matrix

Though this figure does not describe DEGs, it is the first step of a DEG analysis.
The heatplot describes the absolute expression of each gene across the clusters.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_expression(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Kruskal-Wallis

Our chosen method to calculate DEGs will be the non-parametric Kruskal-Wallis test.
The figure displays the top 50 DEGs where $p < 0.01$.

```{r, fig.height = 9}
for (k in all_ks) {
  sc3_plot_de_genes(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Marker Genes

To assign marker genes, a binary classifier is constructed based on the mean cluster expression value.
Classifier prediction is then calculated using the gene expression ranks.
Genes with $AUROC > 0.6$ and $p < 0.01$ are selected and the top 10 are visualized in the following figure.

```{r, fig.height = 7}
markers <- as.data.frame(organise_marker_genes(habib_2017_sce, k = 4, p_val = 0.01, auroc = 0.6))
for (k in all_ks) {
  sc3_plot_markers(habib_2017_sce, k = k, show_pdata = p_data, p.val = 0.01, auroc = 0.6)}
```

## Pathway Analysis

We take marker genes or other interesting DE genes and perform pathway analysis to find enriched pathways.

### g:Profiler

```{r}
gprofiler_sources <- c("GO", "KEGG", "REAC")
```

### Cluster 1

```{r}
keep <- which(markers$sc3_4_markers_clusts == 1)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 2

```{r}
keep <- which(markers$sc3_4_markers_clusts == 2)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 3

```{r}
keep <- which(markers$sc3_4_markers_clusts == 3)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 4

```{r}
keep <- which(markers$sc3_4_markers_clusts == 4)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
rm(gprofiler_out)
```

# Ex Neurons of the PFC

We start by looking in the PFC region and performing deeper clustering on excitatory neurons.

```{r}
# Perform subsetting.
habib_2017_sce <- readRDS(file.path(assets_dir, "habib_2017_sce.rds"))
regions <- "PFC"
cell_types <- "exPFC1|exPFC2"
keep <- grepl(regions, colData(habib_2017_sce)$cell_id_stem)
habib_2017_sce <- habib_2017_sce[, keep]
keep <- grepl(cell_types, colData(habib_2017_sce)$habib_cluster_name)
habib_2017_sce <- habib_2017_sce[, keep]

# Perform PCA.
new_trend <- makeTechTrend(x = habib_2017_sce)
fit <- trendVar(habib_2017_sce, use.spikes = FALSE, loess.args = list(span = 0.05))
fit$trend <- new_trend
dec <- decomposeVar(fit = fit)
rm(fit)
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache2", "habib_2017_pca.rds")
if (file.exists(rds)) {
  habib_2017_pca <- readRDS(rds)
} else {
  habib_2017_pca <- denoisePCA(habib_2017_sce, technical = new_trend, BSPARAM = IrlbaParam())
  saveRDS(habib_2017_pca, rds)}
```

```{r}
habib_2017_sce <- habib_2017_pca
rm(habib_2017_pca)

GLUT <- grepl("GLUT", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GLUT, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GLUT <- as.data.frame(assays(sub_sce)$logcounts)
GLUT <- melt(GLUT, value.name = "GLUT_logcounts")

SATB2 <- grepl("SATB2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[SATB2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
SATB2 <- as.data.frame(assays(sub_sce)$logcounts)
SATB2 <- melt(SATB2, value.name = "SATB2_logcounts")

GRIA1 <- grepl("GRIA1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA1 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA1 <- melt(GRIA1, value.name = "GRIA1_logcounts")

GRIA2 <- grepl("GRIA2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA2 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA2 <- melt(GRIA2, value.name = "GRIA2_logcounts")

WFS1 <- grepl("WFS1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[WFS1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
WFS1 <- as.data.frame(assays(sub_sce)$logcounts)
WFS1 <- melt(WFS1, value.name = "WFS1_logcounts")

TBR1 <- grepl("TBR1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[TBR1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
TBR1 <- as.data.frame(assays(sub_sce)$logcounts)
TBR1 <- melt(TBR1, value.name = "TBR1_logcounts")

rm(sub_sce)

df_redDim <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type")],
  reducedDim(habib_2017_sce, "PCA")[ , 1:2],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  stringsAsFactors = FALSE)
rownames(df_redDim) <- NULL
plotPCA(habib_2017_sce, ncomponents = 3, colour_by = "log10_total_features_by_counts")
```

## Dimensionality Reduction {.tabset}

We use several approaches to identify clusters using tSNE, so this section is broken down into multiple subsections, with each tab referring to a particular set of coordinates.

### Habib tSNE

As tSNE coordinates were released with the original dataset, we plot them here to replicate the results of @habib_massively_2017.

```{r}
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cell_type", type = "cat")
```

### All Genes tSNE

We also calculate and plot new tSNE coordinates using all of the genes in the subsetted dataset.

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache2", "habib_2017_runTSNE.rds")
if (file.exists(rds)) {
  habib_2017_runTSNE <- readRDS(rds)
} else {
  habib_2017_runTSNE <- runTSNE(habib_2017_sce, use_dimred = "PCA", perplexity = 30, rand_seed = 100)
  saveRDS(habib_2017_runTSNE, rds)}
```

```{r}
habib_2017_sce <- habib_2017_runTSNE
tmp_df <- data.frame(reducedDim(habib_2017_sce, "TSNE"), stringsAsFactors = FALSE)
rownames(tmp_df) <- NULL
names(tmp_df) <- paste0("new_tsne", 1:2)
df_redDim <- data.frame(df_redDim, tmp_df, stringsAsFactors = FALSE)

ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cell_type", type = "cat")
rm(df_redDim, tmp_df, habib_2017_runTSNE)
```

### HVG tSNE

We now cluster using the top 1,000 HVGs.

```{r}
# Select the top 1,000 HVGs.
dec1 <- dec
dec1$bio[which(dec$bio < 1e-5)] <- 1e-5
dec1$FDR[which(dec$FDR < 1e-100)] <- 1e-100
w2kp <- which(dec$FDR < 1e-10 & dec$bio > 0.02)
rm(dec, dec1)
habib_2017_sce_hvg <- habib_2017_sce[w2kp, ]
edat <- t(as.matrix(logcounts(habib_2017_sce_hvg)))
edat <- scale(edat)
```

```{r, cache = TRUE}
# Perform PCA.
rds <- file.path(assets_dir, "cache2", "habib_2017_ppk.rds")
if (file.exists(rds)) {
  habib_2017_ppk <- readRDS(rds)
} else {
  habib_2017_ppk <- propack.svd(edat, neig = 50)
  saveRDS(habib_2017_ppk, rds)}
```

```{r}
pca <- t(habib_2017_ppk$d*t(habib_2017_ppk$u))
rm(edat, habib_2017_ppk)
tmp_df <- data.frame(pca[ , 1:2], stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_pc", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type"
  )],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  tmp_df,
  stringsAsFactors = FALSE)
rownames(habib_2017_df_hvg) <- NULL
```

```{r, cache = TRUE}
# Perform tSNE.
rds <- file.path(assets_dir, "cache2", "habib_2017_Rtsne.rds")
if (file.exists(rds)) {
  habib_2017_Rtsne <- readRDS(rds)
} else {
  set.seed(100)
  habib_2017_Rtsne <- Rtsne(pca, pca = FALSE, perplexity = 30)
  saveRDS(habib_2017_Rtsne, rds)}
```

```{r}
tmp_df <- data.frame(habib_2017_Rtsne$Y, stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_tsne", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(habib_2017_df_hvg, tmp_df, stringsAsFactors = FALSE)
reducedDims(habib_2017_sce_hvg) <- SimpleList(PCA = pca, TSNE = habib_2017_Rtsne$Y)
rm(habib_2017_Rtsne, pca, tmp_df)

ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cell_type", type = "cat")
```

## Clustering

### k-means

We apply a k-means method atop the 50 PCs generated earlier, choosing to generate 2-4 clusters.

```{r}
all_num_clust <- c(2:4)
habib_2017_df_hvg <- habib_2017_df_hvg[ , !grepl("^KM_", names(habib_2017_df_hvg))]
rowData(habib_2017_sce_hvg)$feature_symbol <- rowData(habib_2017_sce_hvg)$external_gene_name
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache2", "habib_2017_km_label.rds")
if (file.exists(rds)) {
  km_label <- readRDS(rds)
  habib_2017_df_hvg <- readRDS(file.path(assets_dir, "cache2", "habib_2017_df_hvg1.rds"))
} else {
  for (num_clust in all_num_clust) {
    cat(paste0("k-means with ", num_clust, " clusters.\n"))
    kmeans_out <- kmeans(
      reducedDim(habib_2017_sce_hvg, "PCA"), centers = num_clust, iter.max = 1e8,
      nstart = 2500, algorithm = "MacQueen")
    habib_2017_km_label <- paste0("km_", num_clust, "_clusters")
    habib_2017_df_hvg[[habib_2017_km_label]] = as.factor(kmeans_out$cluster)
    rm(kmeans_out)}
  saveRDS(habib_2017_km_label, rds)
  saveRDS(habib_2017_df_hvg, file.path(assets_dir, "cache2", "habib_2017_df_hvg1.rds"))}
```

### SC3

As an alternative method, we generate another set of 2-4 clusters using SC3.

```{r}
rowData(habib_2017_sce)$feature_symbol <- rowData(habib_2017_sce)$external_gene_name
counts(habib_2017_sce) <- as.matrix(counts(habib_2017_sce))
logcounts(habib_2017_sce) <- as.matrix(logcounts(habib_2017_sce))
all_ks <- c(2:4)
p_data <- c("habib_cluster_name", "cell_id_stem")
```

```{r, cache = TRUE}
habib_2017_sce <- sc3(habib_2017_sce, ks = all_ks, biology = TRUE)
```

```{r}
rds <- file.path(assets_dir, "cache2", "habib_2017_df_hvg2.rds")
if (file.exists(rds)) {
  habib_2017_df_hvg <- readRDS(rds)
  habib_2017_sce <- readRDS(file.path(assets_dir, "cache2", "habib_2017_sce.rds"))
} else {
  for (one_ks in all_ks) {
    sc3_label <- paste0("sc3_", one_ks, "_clusters")
    habib_2017_df_hvg[[sc3_label]] <- as.factor(colData(habib_2017_sce)[, sc3_label])}
  saveRDS(habib_2017_df_hvg, rds)
  saveRDS(habib_2017_sce, file.path(assets_dir, "cache2", "habib_2017_sce.rds"))}
```

## Cluster Mapping {.tabset}

Now we visualize the correlation between our new cluster set as well as the original clusters in @habib_massively_2017.
We start by reading in Supplementary Table 7 (nmeth.4407-S10.xlsx) from @habib_massively_2017 to provide a consistent set of labels and merging it into our data.

```{r}
# Define custom functions.
name_change <- function(data, orig_name, new_name) {
	index = which(names(data) == orig_name)
	if (length(index) > 0) {
		names(data)[index] = new_name
	}
	data}

custom_merge = function(x, y, mess = NULL, ...) {
	if (!is.null(mess)) {
		intersect_vars = paste(intersect(names(x), names(y)), collapse = ", ")
		cat(paste0("Merging dataframes on variables = { ", intersect_vars, " }\n"))
	}
	merge(x, y, by = intersect(names(x), names(y)), ...)}

# Read in supplemental information.
tmp_lab <- read.table(
  file.path(assets_dir, "cluster-num-label.txt"), sep = "\t", header = TRUE, stringsAsFactors = FALSE)
tmp_lab <- name_change(tmp_lab, "Name", "habib_cluster_name")
tmp_lab <- name_change(tmp_lab, "Name.1", "habib_cell_type")
tmp_lab <- name_change(tmp_lab, "Cell.ID", "sample")
tmp_lab <- name_change(tmp_lab, "Cell.Type.ID", "habib_cluster")

tmp_clust <- read.table(
  file.path(assets_dir, "paper-cluster.txt"), sep = "\t", header = TRUE,
  comment.char = "", stringsAsFactors = FALSE)
tmp_clust <- name_change(tmp_clust, "X.Genes", "n_genes")
tmp_clust <- name_change(tmp_clust, "X.Transcripts", "n_transcripts")
tmp_clust <- name_change(tmp_clust, "Cell.ID", "sample")
tmp_clust <- name_change(tmp_clust, "Cluster.ID", "habib_cluster")
tmp_clust <- name_change(tmp_clust, "Cluster.Name", "habib_cluster_name")

tmp_clust <- custom_merge(
  tmp_clust, tmp_lab[ , c("habib_cluster_name", "habib_cell_type")], all.x = TRUE)

# Merge into our data.
habib_2017_all_clust <- custom_merge(habib_2017_df_hvg, tmp_clust)

sc3_clust <- data.frame(
  colData(habib_2017_sce)[ , paste0("sc3_", all_ks, "_clusters")], stringsAsFactors = FALSE)
for (ks in all_ks) {
  sc3_label <- paste0("sc3_", ks, "_clusters")
  sc3_clust[ , sc3_label] <- as.factor(sc3_clust[ , sc3_label])}
sc3_clust$sample <- colnames(habib_2017_sce)

habib_2017_all_clust <- custom_merge(habib_2017_all_clust, sc3_clust)
habib_2017_all_clust <- data.frame(
  habib_2017_all_clust, GLUT["GLUT_logcounts"], SATB2["SATB2_logcounts"], GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"], WFS1["WFS1_logcounts"], TBR1["TBR1_logcounts"], stringsAsFactors = FALSE)
rm(habib_2017_df_hvg, tmp_clust, tmp_lab, sc3_clust, GLUT, GRIA1, GRIA2, SATB2, TBR1, WFS1)
```

We visualize our clusters using tSNE, looking at both the HVG tSNE generated earlier and coloring our clustering results atop of the clusters in @habib_massively_2017.

```{r}
all_vars <- c(
  "habib_cluster_name", "habib_cell_type",
  paste0("km_", all_ks, "_clusters"), paste0("sc3_", all_ks, "_clusters"))
genes <- c(
  "GLUT_logcounts", "SATB2_logcounts", "GRIA1_logcounts",
  "GRIA2_logcounts", "WFS1_logcounts", "TBR1_logcounts")
```

### HVG tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cont"))}
```

### Habib tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cont"))}
```

### Consensus Matrix

We can also assess the performance of our SC3 clusters using a correlation matrix.
In an ideal situation, diagonal blocks will be completely red while off-diagonal elements are completely blue.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_consensus(habib_2017_sce, k = k, show_pdata = p_data)}
```

The diagonality of the concensus matrix can be quantitatively measured using a silhouette plot, where 1 represents a perfectly block-diagonal consensus matrix.

```{r}
# Commented out for now due to errors.
# for (k in all_ks) {
#   sc3_plot_silhouette(habib_2017_sce, k = k)}
```

## Differential Expression

In this section, we calculate differentially expressed genes (DEGs) between the newly created clusters, eventually using this data to assign marker genes to each cluster.

### Expression Matrix

Though this figure does not describe DEGs, it is the first step of a DEG analysis.
The heatplot describes the absolute expression of each gene across the clusters.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_expression(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Kruskal-Wallis

Our chosen method to calculate DEGs will be the non-parametric Kruskal-Wallis test.
The figure displays the top 50 DEGs where $p < 0.01$.

```{r, fig.height = 9}
for (k in all_ks) {
  sc3_plot_de_genes(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Marker Genes

To assign marker genes, a binary classifier is constructed based on the mean cluster expression value.
Classifier prediction is then calculated using the gene expression ranks.
Genes with $AUROC > 0.6$ and $p < 0.01$ are selected and the top 10 are visualized in the following figure.

```{r, fig.height = 7}
markers <- as.data.frame(organise_marker_genes(habib_2017_sce, k = 4, p_val = 0.01, auroc = 0.6))
for (k in all_ks) {
  sc3_plot_markers(habib_2017_sce, k = k, show_pdata = p_data, p.val = 0.01, auroc = 0.6)}
```

## Pathway Analysis

We take marker genes or other interesting DE genes and perform pathway analysis to find enriched pathways.

### g:Profiler

```{r}
gprofiler_sources <- c("GO", "KEGG", "REAC")
```

### Cluster 1

```{r}
keep <- which(markers$sc3_4_markers_clusts == 1)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 2

```{r}
keep <- which(markers$sc3_4_markers_clusts == 2)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 3

```{r}
keep <- which(markers$sc3_4_markers_clusts == 3)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 4

```{r}
keep <- which(markers$sc3_4_markers_clusts == 4)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
rm(gprofiler_out)
```

# In Neurons of the PFC

We start by looking in the PFC region and performing deeper clustering on inhibitory neurons.

```{r}
# Perform subsetting.
habib_2017_sce <- readRDS(file.path(assets_dir, "habib_2017_sce.rds"))
regions <- "PFC"
cell_types <- "GABA1|GABA2"
keep <- grepl(regions, colData(habib_2017_sce)$cell_id_stem)
habib_2017_sce <- habib_2017_sce[, keep]
keep <- grepl(cell_types, colData(habib_2017_sce)$habib_cluster_name)
habib_2017_sce <- habib_2017_sce[, keep]

# Perform PCA.
new_trend <- makeTechTrend(x = habib_2017_sce)
fit <- trendVar(habib_2017_sce, use.spikes = FALSE, loess.args = list(span = 0.05))
fit$trend <- new_trend
dec <- decomposeVar(fit = fit)
rm(fit)
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache3", "habib_2017_pca.rds")
if (file.exists(rds)) {
  habib_2017_pca <- readRDS(rds)
} else {
  habib_2017_pca <- denoisePCA(habib_2017_sce, technical = new_trend, BSPARAM = IrlbaParam())
  saveRDS(habib_2017_pca, rds)}
```

```{r}
habib_2017_sce <- habib_2017_pca
rm(habib_2017_pca)

GLUT <- grepl("GLUT", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GLUT, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GLUT <- as.data.frame(assays(sub_sce)$logcounts)
GLUT <- melt(GLUT, value.name = "GLUT_logcounts")

SATB2 <- grepl("SATB2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[SATB2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
SATB2 <- as.data.frame(assays(sub_sce)$logcounts)
SATB2 <- melt(SATB2, value.name = "SATB2_logcounts")

GRIA1 <- grepl("GRIA1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA1 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA1 <- melt(GRIA1, value.name = "GRIA1_logcounts")

GRIA2 <- grepl("GRIA2", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[GRIA2, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
GRIA2 <- as.data.frame(assays(sub_sce)$logcounts)
GRIA2 <- melt(GRIA2, value.name = "GRIA2_logcounts")

WFS1 <- grepl("WFS1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[WFS1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
WFS1 <- as.data.frame(assays(sub_sce)$logcounts)
WFS1 <- melt(WFS1, value.name = "WFS1_logcounts")

TBR1 <- grepl("TBR1", rowData(habib_2017_sce)$hgnc_symbol)
sub_sce <- habib_2017_sce[TBR1, ]
logcounts(sub_sce) <- as.matrix(logcounts(sub_sce))
TBR1 <- as.data.frame(assays(sub_sce)$logcounts)
TBR1 <- melt(TBR1, value.name = "TBR1_logcounts")

rm(sub_sce)

df_redDim <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type")],
  reducedDim(habib_2017_sce, "PCA")[ , 1:2],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  stringsAsFactors = FALSE)
rownames(df_redDim) <- NULL
plotPCA(habib_2017_sce, ncomponents = 3, colour_by = "log10_total_features_by_counts")
```

## Dimensionality Reduction {.tabset}

We use several approaches to identify clusters using tSNE, so this section is broken down into multiple subsections, with each tab referring to a particular set of coordinates.

### Habib tSNE

As tSNE coordinates were released with the original dataset, we plot them here to replicate the results of @habib_massively_2017.

```{r}
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "habib_tsne1", y = "habib_tsne2", col = "habib_cell_type", type = "cat")
```

### All Genes tSNE

We also calculate and plot new tSNE coordinates using all of the genes in the subsetted dataset.

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache3", "habib_2017_runTSNE.rds")
if (file.exists(rds)) {
  habib_2017_runTSNE <- readRDS(rds)
} else {
  habib_2017_runTSNE <- runTSNE(habib_2017_sce, use_dimred = "PCA", perplexity = 30, rand_seed = 100)
  saveRDS(habib_2017_runTSNE, rds)}
```

```{r}
habib_2017_sce <- habib_2017_runTSNE
tmp_df <- data.frame(reducedDim(habib_2017_sce, "TSNE"), stringsAsFactors = FALSE)
rownames(tmp_df) <- NULL
names(tmp_df) <- paste0("new_tsne", 1:2)
df_redDim <- data.frame(df_redDim, tmp_df, stringsAsFactors = FALSE)

ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = df_redDim, x = "new_tsne1", y = "new_tsne2", col = "habib_cell_type", type = "cat")
rm(df_redDim, tmp_df, habib_2017_runTSNE)
```

### HVG tSNE

We now cluster using the top 1,000 HVGs.

```{r}
# Select the top 1,000 HVGs.
dec1 <- dec
dec1$bio[which(dec$bio < 1e-5)] <- 1e-5
dec1$FDR[which(dec$FDR < 1e-100)] <- 1e-100
w2kp <- which(dec$FDR < 1e-10 & dec$bio > 0.02)
rm(dec, dec1)
habib_2017_sce_hvg <- habib_2017_sce[w2kp, ]
edat <- t(as.matrix(logcounts(habib_2017_sce_hvg)))
edat <- scale(edat)
```

```{r, cache = TRUE}
# Perform PCA.
rds <- file.path(assets_dir, "cache3", "habib_2017_ppk.rds")
if (file.exists(rds)) {
  habib_2017_ppk <- readRDS(rds)
} else {
  habib_2017_ppk <- propack.svd(edat, neig = 50)
  saveRDS(habib_2017_ppk, rds)}
```

```{r}
pca <- t(habib_2017_ppk$d*t(habib_2017_ppk$u))
rm(edat, habib_2017_ppk)
tmp_df <- data.frame(pca[ , 1:2], stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_pc", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(
  colData(habib_2017_sce)[ , c(
    "sample", "cell_id_stem", paste0("habib_tsne", 1:2), "log10_total_features_by_counts",
    "habib_cluster", "habib_cluster_name", "habib_cell_type"
  )],
  GLUT["GLUT_logcounts"],
  SATB2["SATB2_logcounts"],
  GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"],
  WFS1["WFS1_logcounts"],
  TBR1["TBR1_logcounts"],
  tmp_df,
  stringsAsFactors = FALSE)
rownames(habib_2017_df_hvg) <- NULL
```

```{r, cache = TRUE}
# Perform tSNE.
rds <- file.path(assets_dir, "cache3", "habib_2017_Rtsne.rds")
if (file.exists(rds)) {
  habib_2017_Rtsne <- readRDS(rds)
} else {
  set.seed(100)
  habib_2017_Rtsne <- Rtsne(pca, pca = FALSE, perplexity = 30)
  saveRDS(habib_2017_Rtsne, rds)}
```

```{r}
tmp_df <- data.frame(habib_2017_Rtsne$Y, stringsAsFactors = FALSE)
names(tmp_df) <- paste0("hvg_tsne", seq(ncol(tmp_df)))
habib_2017_df_hvg <- data.frame(habib_2017_df_hvg, tmp_df, stringsAsFactors = FALSE)
reducedDims(habib_2017_sce_hvg) <- SimpleList(PCA = pca, TSNE = habib_2017_Rtsne$Y)
rm(habib_2017_Rtsne, pca, tmp_df)

ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "habib_cluster_name", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "cell_id_stem", type = "cat")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GLUT_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "SATB2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "GRIA2_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "WFS1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "TBR1_logcounts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2",
  col = "log10_total_features_by_counts", type = "cont")
ggplot_custom(
  data = habib_2017_df_hvg, x = "hvg_tsne1", y = "hvg_tsne2", col = "habib_cell_type", type = "cat")
```

## Clustering

### k-means

We apply a k-means method atop the 50 PCs generated earlier, choosing to generate 2-4 clusters.

```{r}
all_num_clust <- c(2:4)
habib_2017_df_hvg <- habib_2017_df_hvg[ , !grepl("^KM_", names(habib_2017_df_hvg))]
rowData(habib_2017_sce_hvg)$feature_symbol <- rowData(habib_2017_sce_hvg)$external_gene_name
```

```{r, cache = TRUE}
rds <- file.path(assets_dir, "cache3", "habib_2017_km_label.rds")
if (file.exists(rds)) {
  km_label <- readRDS(rds)
  habib_2017_df_hvg <- readRDS(file.path(assets_dir, "cache3", "habib_2017_df_hvg1.rds"))
} else {
  for (num_clust in all_num_clust) {
    cat(paste0("k-means with ", num_clust, " clusters.\n"))
    kmeans_out <- kmeans(
      reducedDim(habib_2017_sce_hvg, "PCA"), centers = num_clust, iter.max = 1e8,
      nstart = 2500, algorithm = "MacQueen")
    habib_2017_km_label <- paste0("km_", num_clust, "_clusters")
    habib_2017_df_hvg[[habib_2017_km_label]] = as.factor(kmeans_out$cluster)
    rm(kmeans_out)}
  saveRDS(habib_2017_km_label, rds)
  saveRDS(habib_2017_df_hvg, file.path(assets_dir, "cache3", "habib_2017_df_hvg1.rds"))}
```

### SC3

As an alternative method, we generate another set of 2-4 clusters using SC3.

```{r}
rowData(habib_2017_sce)$feature_symbol <- rowData(habib_2017_sce)$external_gene_name
counts(habib_2017_sce) <- as.matrix(counts(habib_2017_sce))
logcounts(habib_2017_sce) <- as.matrix(logcounts(habib_2017_sce))
all_ks <- c(2:4)
p_data <- c("habib_cluster_name", "cell_id_stem")
```

```{r, cache = TRUE}
habib_2017_sce <- sc3(habib_2017_sce, ks = all_ks, biology = TRUE)
```

```{r}
rds <- file.path(assets_dir, "cache3", "habib_2017_df_hvg2.rds")
if (file.exists(rds)) {
  habib_2017_df_hvg <- readRDS(rds)
  habib_2017_sce <- readRDS(file.path(assets_dir, "cache3", "habib_2017_sce.rds"))
} else {
  for (one_ks in all_ks) {
    sc3_label <- paste0("sc3_", one_ks, "_clusters")
    habib_2017_df_hvg[[sc3_label]] <- as.factor(colData(habib_2017_sce)[, sc3_label])}
  saveRDS(habib_2017_df_hvg, rds)
  saveRDS(habib_2017_sce, file.path(assets_dir, "cache3", "habib_2017_sce.rds"))}
```

## Cluster Mapping {.tabset}

Now we visualize the correlation between our new cluster set as well as the original clusters in @habib_massively_2017.
We start by reading in Supplementary Table 7 (nmeth.4407-S10.xlsx) from @habib_massively_2017 to provide a consistent set of labels and merging it into our data.

```{r}
# Define custom functions.
name_change <- function(data, orig_name, new_name) {
	index = which(names(data) == orig_name)
	if (length(index) > 0) {
		names(data)[index] = new_name
	}
	data}

custom_merge = function(x, y, mess = NULL, ...) {
	if (!is.null(mess)) {
		intersect_vars = paste(intersect(names(x), names(y)), collapse = ", ")
		cat(paste0("Merging dataframes on variables = { ", intersect_vars, " }\n"))
	}
	merge(x, y, by = intersect(names(x), names(y)), ...)}

# Read in supplemental information.
tmp_lab <- read.table(
  file.path(assets_dir, "cluster-num-label.txt"), sep = "\t", header = TRUE, stringsAsFactors = FALSE)
tmp_lab <- name_change(tmp_lab, "Name", "habib_cluster_name")
tmp_lab <- name_change(tmp_lab, "Name.1", "habib_cell_type")
tmp_lab <- name_change(tmp_lab, "Cell.ID", "sample")
tmp_lab <- name_change(tmp_lab, "Cell.Type.ID", "habib_cluster")

tmp_clust <- read.table(
  file.path(assets_dir, "paper-cluster.txt"), sep = "\t", header = TRUE,
  comment.char = "", stringsAsFactors = FALSE)
tmp_clust <- name_change(tmp_clust, "X.Genes", "n_genes")
tmp_clust <- name_change(tmp_clust, "X.Transcripts", "n_transcripts")
tmp_clust <- name_change(tmp_clust, "Cell.ID", "sample")
tmp_clust <- name_change(tmp_clust, "Cluster.ID", "habib_cluster")
tmp_clust <- name_change(tmp_clust, "Cluster.Name", "habib_cluster_name")

tmp_clust <- custom_merge(
  tmp_clust, tmp_lab[ , c("habib_cluster_name", "habib_cell_type")], all.x = TRUE)

# Merge into our data.
habib_2017_all_clust <- custom_merge(habib_2017_df_hvg, tmp_clust)

sc3_clust <- data.frame(
  colData(habib_2017_sce)[ , paste0("sc3_", all_ks, "_clusters")], stringsAsFactors = FALSE)
for (ks in all_ks) {
  sc3_label <- paste0("sc3_", ks, "_clusters")
  sc3_clust[ , sc3_label] <- as.factor(sc3_clust[ , sc3_label])}
sc3_clust$sample <- colnames(habib_2017_sce)

habib_2017_all_clust <- custom_merge(habib_2017_all_clust, sc3_clust)
habib_2017_all_clust <- data.frame(
  habib_2017_all_clust, GLUT["GLUT_logcounts"], SATB2["SATB2_logcounts"], GRIA1["GRIA1_logcounts"],
  GRIA2["GRIA2_logcounts"], WFS1["WFS1_logcounts"], TBR1["TBR1_logcounts"], stringsAsFactors = FALSE)
rm(habib_2017_df_hvg, tmp_clust, tmp_lab, sc3_clust, GLUT, GRIA1, GRIA2, SATB2, TBR1, WFS1)
```

We visualize our clusters using tSNE, looking at both the HVG tSNE generated earlier and coloring our clustering results atop of the clusters in @habib_massively_2017.

```{r}
all_vars <- c(
  "habib_cluster_name", "habib_cell_type",
  paste0("km_", all_ks, "_clusters"), paste0("sc3_", all_ks, "_clusters"))
genes <- c(
  "GLUT_logcounts", "SATB2_logcounts", "GRIA1_logcounts",
  "GRIA2_logcounts", "WFS1_logcounts", "TBR1_logcounts")
```

### HVG tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "hvg_tsne1", y = "hvg_tsne2", col = var, type = "cont"))}
```

### Habib tSNE

```{r}
for (var in all_vars) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cat"))}
for (var in genes) {
  print(ggplot_custom(
    data = habib_2017_all_clust, x = "habib_tsne1", y = "habib_tsne2", col = var, type = "cont"))}
```

### Consensus Matrix

We can also assess the performance of our SC3 clusters using a correlation matrix.
In an ideal situation, diagonal blocks will be completely red while off-diagonal elements are completely blue.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_consensus(habib_2017_sce, k = k, show_pdata = p_data)}
```

The diagonality of the concensus matrix can be quantitatively measured using a silhouette plot, where 1 represents a perfectly block-diagonal consensus matrix.

```{r}
# Commented out for now due to errors.
# for (k in all_ks) {
#   sc3_plot_silhouette(habib_2017_sce, k = k)}
```

## Differential Expression

In this section, we calculate differentially expressed genes (DEGs) between the newly created clusters, eventually using this data to assign marker genes to each cluster.

### Expression Matrix

Though this figure does not describe DEGs, it is the first step of a DEG analysis.
The heatplot describes the absolute expression of each gene across the clusters.

```{r, cache = TRUE}
for (k in all_ks) {
  sc3_plot_expression(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Kruskal-Wallis

Our chosen method to calculate DEGs will be the non-parametric Kruskal-Wallis test.
The figure displays the top 50 DEGs where $p < 0.01$.

```{r, fig.height = 9}
for (k in all_ks) {
  sc3_plot_de_genes(habib_2017_sce, k = k, show_pdata = p_data)}
```

### Marker Genes

To assign marker genes, a binary classifier is constructed based on the mean cluster expression value.
Classifier prediction is then calculated using the gene expression ranks.
Genes with $AUROC > 0.6$ and $p < 0.01$ are selected and the top 10 are visualized in the following figure.

```{r, fig.height = 7}
markers <- as.data.frame(organise_marker_genes(habib_2017_sce, k = 4, p_val = 0.01, auroc = 0.6))
for (k in all_ks) {
  sc3_plot_markers(habib_2017_sce, k = k, show_pdata = p_data, p.val = 0.01, auroc = 0.6)}
```

## Pathway Analysis

We take marker genes or other interesting DE genes and perform pathway analysis to find enriched pathways.

### g:Profiler

```{r}
gprofiler_sources <- c("GO", "KEGG", "REAC")
```

### Cluster 1

```{r}
keep <- which(markers$sc3_4_markers_clusts == 1)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 2

```{r}
keep <- which(markers$sc3_4_markers_clusts == 2)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 3

```{r}
keep <- which(markers$sc3_4_markers_clusts == 3)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
```

### Cluster 4

```{r}
keep <- which(markers$sc3_4_markers_clusts == 4)
clust_markers <- markers[keep, ]
gprofiler_out <- gost(
  rownames(clust_markers), organism = "hsapiens", ordered_query = TRUE, sources = gprofiler_sources)
gostplot(gprofiler_out)
```

```{r, fig.width = 11, fig.height = 25}
publish_gosttable(gprofiler_out)
rm(gprofiler_out)
```

# References

This is the concluding section of the document.
Here we write relevant results to disk, output the `session_info`, and create a bibliography for works cited.

```{r}
session_info()
```
